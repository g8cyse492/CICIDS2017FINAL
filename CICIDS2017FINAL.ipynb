{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/g8cyse492/CICIDS2017FINAL/blob/main/CICIDS2017FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApMXhCqkGX2y"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp GeneratedLabelledFlows.zip \"/content/drive/My Drive/CICIDS2017/\""
      ],
      "metadata": {
        "id": "Ao3rCk8bMBB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import codecs\n",
        "import pandas as pd\n",
        "\n",
        "def _to_utf8(filename: str, encoding=\"latin1\", blocksize=1048576):\n",
        "    tmpfilename = filename + \".tmp\"\n",
        "    with codecs.open(filename, \"r\", encoding) as source:\n",
        "        with codecs.open(tmpfilename, \"w\", \"utf-8\") as target:\n",
        "            while True:\n",
        "                contents = source.read(blocksize)\n",
        "                if not contents:\n",
        "                    break\n",
        "                target.write(contents)\n",
        "\n",
        "\n",
        "    os.rename(tmpfilename, filename)"
      ],
      "metadata": {
        "id": "d1D31y_WNFqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = os.path.join(\"/content/drive/My Drive/CICIDS2017/TrafficLabelling\", \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n",
        "\n",
        "_to_utf8(file_name)"
      ],
      "metadata": {
        "id": "xwmcYWq7Q9ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(file_name, skipinitialspace=True, on_bad_lines='skip')\n",
        "\n",
        "\n",
        "print(\"Removing {} rows that contains only NaN values...\".format(df[df.isna().all(axis=1)].shape[0]))\n",
        "\n",
        "\n",
        "df = df[~ df.isna().all(axis=1)]"
      ],
      "metadata": {
        "id": "2bCeA2SZRMMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _renaming_class_label(df: pd.DataFrame):\n",
        "    labels = {\"Web Attack \\x96 Brute Force\": \"Web Attack-Brute Force\",\n",
        "              \"Web Attack \\x96 XSS\": \"Web Attack-XSS\",\n",
        "              \"Web Attack \\x96 Sql Injection\": \"Web Attack-Sql Injection\"}\n",
        "\n",
        "    for old_label, new_label in labels.items():\n",
        "        df.Label.replace(old_label, new_label, inplace=True)\n",
        "\n",
        "\n",
        "_renaming_class_label(df)"
      ],
      "metadata": {
        "id": "qIXw9wKqRtvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.to_csv(file_name, index=False)"
      ],
      "metadata": {
        "id": "_zCaSXvrSAkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DIR_PATH = \"/content/drive/My Drive/CICIDS2017/TrafficLabelling\"\n",
        "\n",
        "FILE_NAMES = [\"Monday-WorkingHours.pcap_ISCX.csv\",\n",
        "              \"Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
        "              \"Wednesday-workingHours.pcap_ISCX.csv\",\n",
        "              \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
        "              \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
        "              \"Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
        "              \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
        "              \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"]\n",
        "df = [pd.read_csv(os.path.join(DIR_PATH, f), skipinitialspace=True) for f in FILE_NAMES]\n",
        "df = pd.concat(df, ignore_index=True)\n",
        "df.Label.value_counts()"
      ],
      "metadata": {
        "id": "1byhe4XcSKnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.to_csv(os.path.join(DIR_PATH, \"TrafficLabelling.csv\"), index=False)"
      ],
      "metadata": {
        "id": "KZcWWexrTKCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "file_name = os.path.join(\"/content/drive/My Drive/CICIDS2017/MachineLearningCSV/MachineLearningCVE\", \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n",
        "\n",
        "df = pd.read_csv(file_name, skipinitialspace=True, on_bad_lines='skip')"
      ],
      "metadata": {
        "id": "RBUv5RCIpxny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _renaming_class_label(df: pd.DataFrame):\n",
        "    labels = {\"Web Attack � Brute Force\": \"Web Attack-Brute Force\",\n",
        "              \"Web Attack � XSS\": \"Web Attack-XSS\",\n",
        "              \"Web Attack � Sql Injection\": \"Web Attack-Sql Injection\"}\n",
        "\n",
        "    for old_label, new_label in labels.items():\n",
        "        df.Label.replace(old_label, new_label, inplace=True)\n",
        "\n",
        "_renaming_class_label(df)"
      ],
      "metadata": {
        "id": "Yh5JU1v0q-c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.to_csv(file_name, index=False)"
      ],
      "metadata": {
        "id": "8hAiJcEUsw3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DIR_PATH = \"/content/drive/My Drive/CICIDS2017/MachineLearningCSV/MachineLearningCVE\"\n",
        "\n",
        "FILE_NAMES = [\"Monday-WorkingHours.pcap_ISCX.csv\",\n",
        "              \"Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
        "              \"Wednesday-workingHours.pcap_ISCX.csv\",\n",
        "              \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
        "              \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
        "              \"Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
        "              \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
        "              \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"]"
      ],
      "metadata": {
        "id": "9lRcvfCFtQ4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = [pd.read_csv(os.path.join(DIR_PATH, f), skipinitialspace=True) for f in FILE_NAMES]\n",
        "df = pd.concat(df, ignore_index=True)\n",
        "df.Label.value_counts()"
      ],
      "metadata": {
        "id": "OUm0D3yrtV9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(os.path.join(DIR_PATH, \"MachineLearningCVE.csv\"), index=False)"
      ],
      "metadata": {
        "id": "Ru-Dj1LdttYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "logging.basicConfig(format=\"%(asctime)s %(levelname)s %(message)s\", datefmt=\"%H:%M:%S\", level=logging.INFO)\n",
        "\n",
        "pd.set_option(\"display.max_rows\", 85)\n",
        "\n",
        "TRAFFICLABELLING_PATH = \"/content/drive/My Drive/CICIDS2017/TrafficLabelling/TrafficLabelling.csv\"\n",
        "MACHINELEARNINGCVE_PATH = \"/content/drive/My Drive/CICIDS2017/MachineLearningCSV/MachineLearningCVE/MachineLearningCVE.csv\"\n",
        "\n",
        "TrafficLabelling = pd.read_csv(TRAFFICLABELLING_PATH, skipinitialspace=True)\n",
        "MachineLearningCVE = pd.read_csv(MACHINELEARNINGCVE_PATH, skipinitialspace=True)"
      ],
      "metadata": {
        "id": "mzFvS2aSupzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrafficLabelling.Label.value_counts()"
      ],
      "metadata": {
        "id": "NGgoKdfA2zHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MachineLearningCVE.Label.value_counts()"
      ],
      "metadata": {
        "id": "n8GhEd0U25HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TrafficLabelling.shape"
      ],
      "metadata": {
        "id": "va8FIVzm29K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MachineLearningCVE.shape"
      ],
      "metadata": {
        "id": "KRu0Piyw3B-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.setdiff1d(TrafficLabelling.columns, MachineLearningCVE.columns)"
      ],
      "metadata": {
        "id": "dPFDS1pW3Iw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrafficLabelling.Protocol.value_counts()"
      ],
      "metadata": {
        "id": "0ENNxPIq3MK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import logging\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "logging.basicConfig(format=\"%(asctime)s %(levelname)s %(message)s\", datefmt=\"%H:%M:%S\", level=logging.INFO)\n",
        "\n",
        "\n",
        "pd.set_option(\"display.max_rows\", 85)"
      ],
      "metadata": {
        "id": "tY4TI4L13PHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_PATH = \"/content/drive/My Drive/CICIDS2017/MachineLearningCSV/MachineLearningCVE\"\n",
        "PROCESSED_DIR_PATH = \"/content/drive/My Drive/CICIDS2017/ProcessedDataset\"\n",
        "FILE_PATH = \"/content/drive/My Drive/CICIDS2017/MachineLearningCSV/MachineLearningCVE/MachineLearningCVE.csv\"\n",
        "\n",
        "if os.path.isfile(FILE_PATH):\n",
        "    print(f\"File exists: {FILE_PATH}\")\n",
        "else:\n",
        "    print(f\"File not found: {FILE_PATH}\")\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    labels = pd.read_csv(FILE_PATH, usecols=['Label'], skipinitialspace=True)\n",
        "    print(labels.head())\n",
        "    print(f\"Shape: {labels.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "if not os.path.exists(PROCESSED_DIR_PATH):\n",
        "    os.makedirs(PROCESSED_DIR_PATH)\n",
        "    print(f\"Created directory: {PROCESSED_DIR_PATH}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {PROCESSED_DIR_PATH}\")\n"
      ],
      "metadata": {
        "id": "jR-QxxIj3YBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "logging.info(\"Reading file...\")\n",
        "\n",
        "def _label_encoding() -> LabelEncoder:\n",
        "\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    try:\n",
        "\n",
        "        print(\"Reading file...\")\n",
        "        labels = pd.read_csv(FILE_PATH, usecols=['Label'], skipinitialspace=True)\n",
        "        print(f\"Total rows read: {labels.shape}\")\n",
        "\n",
        "        le.fit(labels.Label)\n",
        "        print(f\"Labels fitted: {len(le.classes_)} classes found.\")\n",
        "\n",
        "        np.save(os.path.join(PROCESSED_DIR_PATH, 'label_encoder.npy'), le.classes_)\n",
        "        print(\"Label encoder saved.\")\n",
        "\n",
        "        print(f\"Class distribution:\\n{labels.Label.value_counts()}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in _label_encoding(): {e}\")\n",
        "        raise\n",
        "\n",
        "    return le\n",
        "\n",
        "\n",
        "\n",
        "def _process(df: pd.DataFrame, le: LabelEncoder) -> pd.DataFrame:\n",
        "\n",
        "    df.Label = le.transform(df.Label)\n",
        "\n",
        "\n",
        "    nan_rows = df[df.isna().any(axis=1)].shape[0]\n",
        "    logging.info(f\"Fill NaN in {nan_rows} rows with average value of each class.\")\n",
        "    df.iloc[:, df.columns != \"Label\"] = df.groupby(\"Label\").transform(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "\n",
        "    inf_rows = df[df.isin([np.inf]).any(axis=1)].shape[0]\n",
        "    logging.info(f\"Replace Inf in {inf_rows} rows with maximum value of each class.\")\n",
        "    df.replace([np.inf], np.nan, inplace=True)\n",
        "    df.iloc[:, df.columns != \"Label\"] = df.groupby(\"Label\").transform(lambda x: x.fillna(x.max()))\n",
        "\n",
        "\n",
        "    logging.info(\"Replace negative values with minimum value of each class.\")\n",
        "    df[df < 0] = np.nan\n",
        "    df.iloc[:, df.columns != \"Label\"] = df.groupby(\"Label\").transform(lambda x: x.fillna(x.min()))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def _split_train_test(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
        "\n",
        "    x = df.iloc[:, df.columns != 'Label']\n",
        "    y = df[['Label']]\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.20,\n",
        "                                                        random_state=np.random.randint(10))\n",
        "\n",
        "    del x, y\n",
        "\n",
        "    train = pd.concat([x_train, y_train], axis=1, sort=False)\n",
        "    test = pd.concat([x_test, y_test], axis=1, sort=False)\n",
        "\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def _to_csv(df: pd.DataFrame, saving_path: str):\n",
        "\n",
        "    if not os.path.isfile(saving_path):\n",
        "        df.to_csv(saving_path, index=False)\n",
        "\n",
        "    else:\n",
        "        df.to_csv(saving_path, index=False, mode='a', header=False)\n",
        "\n",
        "\n",
        "def _preprocessing_all(le: LabelEncoder, chunksize=2500000):\n",
        "    for chunk in pd.read_csv(FILE_PATH, skipinitialspace=True, chunksize=chunksize):\n",
        "        print(f\"Processing chunk with shape: {chunk.shape}\")\n",
        "        train, test = _split_train_test(_process(chunk, le))\n",
        "        _to_csv(train, os.path.join(PROCESSED_DIR_PATH, \"train_MachineLearningCVE.csv\"))\n",
        "        print(\"Saved train chunk.\")\n",
        "        _to_csv(test, os.path.join(PROCESSED_DIR_PATH, \"test_MachineLearningCVE.csv\"))\n",
        "        print(\"Saved test chunk.\")\n"
      ],
      "metadata": {
        "id": "DiKDt-qp3cze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = _label_encoding()"
      ],
      "metadata": {
        "id": "zsRDHC9e3eQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "_preprocessing_all(label_encoder, 2500000)"
      ],
      "metadata": {
        "id": "kqiCtiMe9D-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.info(\"*** END ***\")"
      ],
      "metadata": {
        "id": "b9VZA9n79bWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning torchviz seaborn\n",
        "\n",
        "# Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from torchviz import make_dot\n",
        "from IPython.display import Image, display\n",
        "import seaborn as sns\n",
        "\n",
        "###############################################################################\n",
        "# 1. Set seeds for reproducibility\n",
        "###############################################################################\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # if using GPUs\n",
        "\n",
        "###############################################################################\n",
        "# Logging and Pandas display configuration\n",
        "###############################################################################\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    level=logging.INFO\n",
        ")\n",
        "pd.set_option(\"display.max_rows\", 85)\n",
        "\n",
        "###############################################################################\n",
        "# Data Preprocessing\n",
        "###############################################################################\n",
        "def preprocessing(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)      # Shuffle and reset index\n",
        "    x = df.loc[:, df.columns != 'Label']                                  # Separate features\n",
        "    y = df[['Label']].values.flatten()                                    # Separate labels\n",
        "    scaler = MinMaxScaler()                                               # Scale features to [0, 1]\n",
        "    x = scaler.fit_transform(x)                                           # Fit scaler on features and transform\n",
        "    return x, y                                                           # return scaled features and labels\n",
        "\n",
        "###############################################################################\n",
        "# Callback to log and store training metrics\n",
        "###############################################################################\n",
        "class MetricsLogger(pl.Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()                                                                      # Initialize parent Callback class\n",
        "        self.history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}       # Store history for training and validation metrics\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        metrics = trainer.callback_metrics                                                         # Get metrics from the current epoch\n",
        "\n",
        "        self.history[\"train_loss\"].append(                                                         # Add training loss for the epoch to history\n",
        "            metrics.get(\"train_loss\").item() if metrics.get(\"train_loss\") is not None else None\n",
        "        )\n",
        "        self.history[\"train_acc\"].append(                                                          # Add training accuracy for the epoch to history\n",
        "            metrics.get(\"train_acc\").item() if metrics.get(\"train_acc\") is not None else None\n",
        "        )\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer, pl_module):\n",
        "        metrics = trainer.callback_metrics                                                         # Get metrics from validation epoch\n",
        "        self.history[\"val_loss\"].append(\n",
        "            metrics.get(\"val_loss\").item() if metrics.get(\"val_loss\") is not None else None        # Add validation loss for epoch to history.\n",
        "        )\n",
        "        self.history[\"val_acc\"].append(                                                            # Add validation accuracy for epoch to history\n",
        "            metrics.get(\"val_acc\").item() if metrics.get(\"val_acc\") is not None else None\n",
        "        )\n",
        "\n",
        "###############################################################################\n",
        "# Plot training history\n",
        "###############################################################################\n",
        "def plot_history(history: dict):\n",
        "    # Plot training and validation accuracy over epochs\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(history[\"train_acc\"], label=\"Train Accuracy\", marker=\"o\")\n",
        "    plt.plot(history[\"val_acc\"], label=\"Validation Accuracy\", marker=\"o\")\n",
        "    plt.title(\"Model Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training and validation loss over epochs\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"Validation Loss\", marker=\"o\")\n",
        "    plt.title(\"Model Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# Evaluation with confusion matrix\n",
        "###############################################################################\n",
        "def evaluation(model: pl.LightningModule, dataloader: DataLoader):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device  # Model's current device\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            # Move data to the same device as the model to avoid CPU/GPU mismatch\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "    print(\"Evaluation:\\n{}\".format(classification_report(all_targets, all_preds)))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# Define Focal Loss\n",
        "###############################################################################\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss as described in https://arxiv.org/abs/1708.02002\n",
        "    Default: alpha=1.0, gamma=2.0\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha                                            # Weighting factor for balancing\n",
        "        self.gamma = gamma                                            # Focusing parameter for hard examples\n",
        "        self.reduction = reduction                                    # Reduction method: 'mean', 'sum', or 'none'\n",
        "        self.cross_entropy = nn.CrossEntropyLoss(reduction='none')    # Compute loss per sample\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = self.cross_entropy(inputs, targets)           # Compute the per-sample cross-entropy loss\n",
        "        p_t = torch.exp(-ce_loss)                               # Compute the probability of the true class for each sample\n",
        "        modulating_factor = (1 - p_t) ** self.gamma             # Compute the modulating factor for each sample\n",
        "        focal_loss = self.alpha * modulating_factor * ce_loss   # Compute the focal loss by scaling the cross-entropy loss\n",
        "\n",
        "        if self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        elif self.reduction == 'none':\n",
        "            return focal_loss\n",
        "        else:\n",
        "            return focal_loss.mean()      # Default to mean reduction\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# PyTorch Lightning Model\n",
        "###############################################################################\n",
        "class DenseClassifier(pl.LightningModule):\n",
        "    def __init__(self, input_dim=78, num_classes=15, lr=1e-3):\n",
        "        super(DenseClassifier, self).__init__()\n",
        "        self.save_hyperparameters()           # Save hyperparameters (input_dim, num_classes, lr) for reproducibility\n",
        "\n",
        "        # Define individual layers of the network:\n",
        "        self.fc1 = nn.Linear(input_dim, 128)        # Fully connected layer: input -> 128 neurons\n",
        "        self.relu1 = nn.ReLU()                      # ReLU activation, introduces non-linearity while maintaining computational efficiency\n",
        "\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.fc4 = nn.Linear(32, num_classes)       # Output layer: 32 -> num_classes\n",
        "\n",
        "        # Combine all layers to sequential model for streamlined forward pass\n",
        "        self.net = nn.Sequential(\n",
        "            self.fc1, self.relu1,\n",
        "            self.fc2, self.relu2,\n",
        "            self.fc3, self.relu3,\n",
        "            self.fc4\n",
        "        )\n",
        "\n",
        "        self.loss_fn = FocalLoss(alpha=1.0, gamma=2.0)     # Set loss function to FocalLoss for class imbalance\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)      # Pass input through sequential layers\n",
        "\n",
        "    def step(self, batch, stage=\"train\"):\n",
        "        inputs, labels = batch                                # Unpack batch into inputs and labels\n",
        "\n",
        "        logits = self(inputs)                                 # Forward pass: compute raw output (logits)\n",
        "        loss = self.loss_fn(logits, labels)                   # Compute loss using FocalLoss\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=1)             # Determine predicted class for each sample\n",
        "        accuracy = (predictions == labels).float().mean()     # Compute accuracy\n",
        "\n",
        "        self.log(f\"{stage}_loss\", loss, on_epoch=True, prog_bar=True)      # Log loss with prefix based stage train\n",
        "        self.log(f\"{stage}_acc\", accuracy, on_epoch=True, prog_bar=True)   # Log accuracy with prefix based stage val\n",
        "\n",
        "        return loss          # Return loss for backpropagation in training\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.step(batch, stage=\"train\")      # Processes single training batch\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self.step(batch, stage=\"val\")               # Processes single validation batch\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)     # Initialize Adam optimizer with learning rate from hyperparameters\n",
        "\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(                 # Define scheduler to reduce learning rate when validation accuracy plateaus\n",
        "            optimizer, mode=\"max\", factor=0.1, patience=5, verbose=True\n",
        "        )\n",
        "\n",
        "        return {                                       # Return optimizer and scheduler in dictionary format\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"monitor\": \"val_acc\",                  # Scheduler monitors validation accuracy to adjust learning rate\n",
        "                \"interval\": \"epoch\",\n",
        "                \"frequency\": 1,\n",
        "            }\n",
        "        }\n",
        "\n",
        "###############################################################################\n",
        "# Main code\n",
        "###############################################################################\n",
        "PROCESSED_DIR_PATH = \"/content/drive/My Drive/CICIDS2017/ProcessedDataset\"\n",
        "MODEL_DIR_PATH = \"/content/drive/My Drive/CICIDS2017/Model\"\n",
        "\n",
        "\n",
        "def graph_visualize(model, input_dim=78, output_filename=\"model_arch\"):\n",
        "    dummy_input = torch.randn(1, input_dim)                           # Create a dummy input tensor\n",
        "    dot = make_dot(model(dummy_input), params=dict(model.named_parameters()))\n",
        "    dot.format = \"png\"\n",
        "    dot.render(output_filename, cleanup=True)\n",
        "    display(Image(f\"{output_filename}.png\"))                          # Display the generated image\n",
        "\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    csv_file = os.path.join(PROCESSED_DIR_PATH, 'train_MachineLearningCVE.csv')     # Load the training CSV file\n",
        "    df = pd.read_csv(csv_file, skipinitialspace=True)                               # Read csv file\n",
        "    logging.info(\"Class distribution:\\n{}\".format(df.Label.value_counts()))         # Log class distribution\n",
        "\n",
        "    X, y = preprocessing(df)\n",
        "    del df                      # Free up memory, delete DataFrame\n",
        "\n",
        "    # Convert processed data into PyTorch tensors\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "    return X_tensor, y_tensor\n",
        "\n",
        "\n",
        "def data_loaders(X_tensor, y_tensor, batch_size=1024):\n",
        "    train_X, val_X, train_y, val_y = train_test_split(                          # Split data: 90% for training, 10% for validation\n",
        "        X_tensor, y_tensor, test_size=0.1, random_state=SEED, stratify=y_tensor\n",
        "    )\n",
        "\n",
        "    train_dataset = TensorDataset(train_X, train_y)                   # Create TensorDataset objects for train sets\n",
        "    val_dataset = TensorDataset(val_X, val_y)                         # Create TensorDataset objects for validation sets\n",
        "\n",
        "\n",
        "    train_labels = train_y.numpy()\n",
        "    class_counts = np.bincount(train_labels)                          # Calculate class counts for training labels\n",
        "    logging.info(\"Training class counts: {}\".format(class_counts))\n",
        "\n",
        "    class_weights = 1.0 / (class_counts ** 0.4)                       # Calculate class_weight\n",
        "    sample_weights = class_weights[train_labels]                      # Assign weigth to each sample based on class label\n",
        "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)              # Create WeightedRandomSampler for training Dataloader\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)         # Create DataLoaders for training datasets\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)                              # Create DataLoaders for validation datasets\n",
        "    return train_loader, val_loader                                                          # Return DataLoaders with weighted sampling\n",
        "\n",
        "\n",
        "def configure_callbacks():\n",
        "    metrics_logger = MetricsLogger()                          # Logs training and validation metrics.\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(       # Saves the best model based on validation accuracy\n",
        "        dirpath = MODEL_DIR_PATH,\n",
        "        filename = \"best_dense_{epoch:02d}-{val_acc:.4f}\",\n",
        "        monitor = \"val_acc\",\n",
        "        mode = \"max\",\n",
        "        save_top_k = 1\n",
        "    )\n",
        "    return metrics_logger, checkpoint_callback\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    model = DenseClassifier()                                   # Create an instance of the model\n",
        "    logging.info(\"Model architecture:\\n{}\".format(model))\n",
        "\n",
        "    graph_visualize(model, input_dim=78)           # Visualize the model architecture\n",
        "\n",
        "    X_tensor, y_tensor = load_and_prepare_data()                # Load, preprocess, and convert training data into tensors\n",
        "\n",
        "    train_loader, val_loader = data_loaders(X_tensor, y_tensor, batch_size=1024)    # Create DataLoaders for training and validation\n",
        "\n",
        "    metrics_logger, checkpoint_callback = configure_callbacks()       # Set up callbacks for logging and checkpointing\n",
        "\n",
        "    trainer = pl.Trainer(                             # Configure PyTorch Lightning Trainer\n",
        "        max_epochs=125,\n",
        "        callbacks=[metrics_logger, checkpoint_callback],\n",
        "        log_every_n_steps=10,\n",
        "        accelerator='auto'\n",
        "    )\n",
        "\n",
        "    logging.info(\"*** TRAINING START ***\")\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    logging.info(\"*** TRAINING FINISH ***\")\n",
        "\n",
        "    global best_model\n",
        "    best_checkpoint_path = checkpoint_callback.best_model_path                  # Retrieve and log path of best model checkpoint\n",
        "    logging.info(f\"Best model saved at: {best_checkpoint_path}\")\n",
        "    best_model = DenseClassifier.load_from_checkpoint(best_checkpoint_path)     # Load best model from checkpoint\n",
        "\n",
        "    plot_history(metrics_logger.history)            # Plot training history (accuracy and loss over epochs)\n",
        "\n",
        "\n",
        "    eval_dataset = TensorDataset(X_tensor, y_tensor)            # Evaluate dataset\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=1024)     # Evaluate data loader\n",
        "    evaluation(best_model, eval_loader)                         # Evaluate the model on the entire dataset\n",
        "\n",
        "\n",
        "# Entry point for the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "fTOBSqk1Dfft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# LABEL POST EVALUATION CSV\n",
        "###############################################################################\n",
        "def label_csv(model: pl.LightningModule, input_csv_path: str, output_csv_path: str, class_mapping: dict) -> pd.DataFrame:\n",
        "    df = pd.read_csv(input_csv_path)              # Load the CSV file\n",
        "\n",
        "    numeric_columns = df.select_dtypes(include=['number']).columns      # Select only numeric columns (features)\n",
        "    unlabeled_features = df[numeric_columns]                            # Create new DataFrame containing only numeric features.\n",
        "\n",
        "    #unlabeled_features = unlabeled_features.replace([np.inf, -np.inf], np.nan).fillna(0)    # Replace infinite values with NaN and then fill NaNs with 0.\n",
        "\n",
        "\n",
        "    if unlabeled_features.shape[1] != model.hparams.input_dim:          # Ensure number of features matches model's input dimension\n",
        "        raise ValueError(\n",
        "            f\"Number of features in the dataset ({unlabeled_features.shape[1]}) does not match \"\n",
        "            f\"the model's input dimension ({model.hparams.input_dim}).\"\n",
        "        )\n",
        "\n",
        "\n",
        "    scaler = MinMaxScaler()                                     # Create instance\n",
        "    X_scaled = scaler.fit_transform(unlabeled_features)         # Preprocess data (scale to [0,1])\n",
        "\n",
        "    X_tensor = torch.tensor(X_scaled, dtype=torch.float32)      # Convert to PyTorch tensor\n",
        "\n",
        "    model.eval()                                                # Ensure model in evaluation mode\n",
        "    device = next(model.parameters()).device                    # Get model device\n",
        "    X_tensor = X_tensor.to(device)                              # Move tensor to same device as the model\n",
        "\n",
        "    with torch.no_grad():                                       # Predict labels\n",
        "        logits = model(X_tensor)\n",
        "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "    df[\"Predicted_Label\"] = [class_mapping[pred] for pred in predictions]     # Map predicted class indices to class names\n",
        "\n",
        "    df.to_csv(output_csv_path, index=False)               # Save labeled CSV\n",
        "    print(f\"Labeled dataset saved to: {output_csv_path}\")\n",
        "    return df\n",
        "\n",
        "###############################################################################\n",
        "# HELPER FUNCTION: keep columns found in training set\n",
        "###############################################################################\n",
        "def drop_unwanted_columns(df: pd.DataFrame, train_cols: list) -> pd.DataFrame:\n",
        "    common_cols = [c for c in df.columns if c in train_cols]        # Intersect columns with train_cols\n",
        "\n",
        "    df = df[common_cols]                                            # Re-order exactly as train_cols (dropping any missing)\n",
        "\n",
        "    missing_cols = set(train_cols) - set(common_cols)               # Identify missing columns\n",
        "    for col in missing_cols:                                        # Fill missing columns with 0\n",
        "        df.loc[:, col] = 0\n",
        "    df = df[train_cols]                                             # Re-order exactly as train_cols (dropping any missing)\n",
        "    return df\n",
        "\n",
        "###############################################################################\n",
        "# MAIN LOOP\n",
        "###############################################################################\n",
        "unlabeled_folder = \"/content/drive/My Drive/CSV_UNLABELED\"\n",
        "labeled_folder   = \"/content/drive/My Drive/CSV_LABELED\"\n",
        "\n",
        "train_df = pd.read_csv(                                                 # Load the training dataset\n",
        "    os.path.join(PROCESSED_DIR_PATH, 'train_MachineLearningCVE.csv'),\n",
        "    skipinitialspace=True\n",
        ")\n",
        "\n",
        "train_columns = train_df.drop(columns=['Label']).columns.tolist()     # Grab columns used by model (exclude 'Label')\n",
        "\n",
        "column_mapping = {                      # Column rename dictionary\n",
        "    \"Src Port\": \"Source Port\",\n",
        "    \"Dst Port\": \"Destination Port\",\n",
        "    \"Tot Fwd Pkts\": \"Total Fwd Packets\",\n",
        "    \"Tot Bwd Pkts\": \"Total Backward Packets\",\n",
        "    \"TotLen Fwd Pkts\": \"Total Length of Fwd Packets\",\n",
        "    \"TotLen Bwd Pkts\": \"Total Length of Bwd Packets\",\n",
        "    \"Fwd Pkt Len Max\": \"Fwd Packet Length Max\",\n",
        "    \"Fwd Pkt Len Min\": \"Fwd Packet Length Min\",\n",
        "    \"Fwd Pkt Len Mean\": \"Fwd Packet Length Mean\",\n",
        "    \"Fwd Pkt Len Std\": \"Fwd Packet Length Std\",\n",
        "    \"Bwd Pkt Len Max\": \"Bwd Packet Length Max\",\n",
        "    \"Bwd Pkt Len Min\": \"Bwd Packet Length Min\",\n",
        "    \"Bwd Pkt Len Mean\": \"Bwd Packet Length Mean\",\n",
        "    \"Bwd Pkt Len Std\": \"Bwd Packet Length Std\",\n",
        "    \"Flow Byts/s\": \"Flow Bytes/s\",\n",
        "    \"Flow Pkts/s\": \"Flow Packets/s\",\n",
        "    \"Fwd IAT Tot\": \"Fwd IAT Total\",\n",
        "    \"Bwd IAT Tot\": \"Bwd IAT Total\",\n",
        "    \"Fwd Header Len\": \"Fwd Header Length\",\n",
        "    \"Bwd Header Len\": \"Bwd Header Length\",\n",
        "    \"Fwd Pkts/s\": \"Fwd Packets/s\",\n",
        "    \"Bwd Pkts/s\": \"Bwd Packets/s\",\n",
        "    \"Pkt Len Min\": \"Min Packet Length\",\n",
        "    \"Pkt Len Max\": \"Max Packet Length\",\n",
        "    \"Pkt Len Mean\": \"Packet Length Mean\",\n",
        "    \"Pkt Len Std\": \"Packet Length Std\",\n",
        "    \"Pkt Len Var\": \"Packet Length Variance\",\n",
        "    \"FIN Flag Cnt\": \"FIN Flag Count\",\n",
        "    \"SYN Flag Cnt\": \"SYN Flag Count\",\n",
        "    \"RST Flag Cnt\": \"RST Flag Count\",\n",
        "    \"PSH Flag Cnt\": \"PSH Flag Count\",\n",
        "    \"ACK Flag Cnt\": \"ACK Flag Count\",\n",
        "    \"URG Flag Cnt\": \"URG Flag Count\",\n",
        "    \"CWE Flag Count\": \"CWE Flag Count\",\n",
        "    \"ECE Flag Cnt\": \"ECE Flag Count\",\n",
        "    \"Pkt Size Avg\": \"Average Packet Size\",\n",
        "    \"Fwd Seg Size Avg\": \"Avg Fwd Segment Size\",\n",
        "    \"Bwd Seg Size Avg\": \"Avg Bwd Segment Size\",\n",
        "    \"Fwd Byts/b Avg\": \"Fwd Avg Bytes/Bulk\",\n",
        "    \"Fwd Pkts/b Avg\": \"Fwd Avg Packets/Bulk\",\n",
        "    \"Fwd Blk Rate Avg\": \"Fwd Avg Bulk Rate\",\n",
        "    \"Bwd Byts/b Avg\": \"Bwd Avg Bytes/Bulk\",\n",
        "    \"Bwd Pkts/b Avg\": \"Bwd Avg Packets/Bulk\",\n",
        "    \"Bwd Blk Rate Avg\": \"Bwd Avg Bulk Rate\",\n",
        "    \"Subflow Fwd Pkts\": \"Subflow Fwd Packets\",\n",
        "    \"Subflow Fwd Byts\": \"Subflow Fwd Bytes\",\n",
        "    \"Subflow Bwd Pkts\": \"Subflow Bwd Packets\",\n",
        "    \"Subflow Bwd Byts\": \"Subflow Bwd Bytes\",\n",
        "    \"Init Fwd Win Byts\": \"Init_Win_bytes_forward\",\n",
        "    \"Init Bwd Win Byts\": \"Init_Win_bytes_backward\",\n",
        "    \"Fwd Act Data Pkts\": \"act_data_pkt_fwd\",\n",
        "    \"Fwd Seg Size Min\": \"min_seg_size_forward\"\n",
        "}\n",
        "\n",
        "class_mapping = {             # Class mapping for final predictions\n",
        "    0:  \"Benign\",\n",
        "    1:  \"Bot\",\n",
        "    2:  \"DDoS\",\n",
        "    3:  \"DoS GoldenEye\",\n",
        "    4:  \"DoS Hulk\",\n",
        "    5:  \"DoS Slowhttptest\",\n",
        "    6:  \"DoS slowloris\",\n",
        "    7:  \"FTP-Patator\",\n",
        "    8:  \"Heartbleed\",\n",
        "    9:  \"Infiltration\",\n",
        "    10: \"PortScan\",\n",
        "    11: \"SSH-Patator\",\n",
        "    12: \"Web Attack-Brute Force\",\n",
        "    13: \"Web Attack-Sql Injection\",\n",
        "    14: \"Web Attack-XSS\"\n",
        "}\n",
        "\n",
        "for filename in os.listdir(unlabeled_folder):\n",
        "    if filename.lower().endswith(\".csv\"):\n",
        "        input_csv_path = os.path.join(unlabeled_folder, filename)\n",
        "        base_name = os.path.splitext(filename)[0]\n",
        "        output_csv_path = os.path.join(labeled_folder, base_name + \"_labeled.csv\")\n",
        "\n",
        "        df = pd.read_csv(input_csv_path)                    # Load the unlabeled dataset\n",
        "        df = df.rename(columns=column_mapping)              # Rename columns if they appear in your dictionary\n",
        "        df = drop_unwanted_columns(df, train_columns)       # Drop colums not in train_columns and ensure they are in correct order.\n",
        "        df.to_csv(input_csv_path, index=False)              # Overwrite CSV with the corrected columns\n",
        "        labeled_df = label_csv(best_model, input_csv_path, output_csv_path, class_mapping)     # Label dataset"
      ],
      "metadata": {
        "id": "bg2UAYWvtbof"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}