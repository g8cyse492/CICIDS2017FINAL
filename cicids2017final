from google.colab import drive
drive.mount("/content/drive")

!cp GeneratedLabelledFlows.zip "/content/drive/My Drive/CICIDS2017/"

import os
import codecs
import pandas as pd

def _to_utf8(filename: str, encoding="latin1", blocksize=1048576):
    tmpfilename = filename + ".tmp"
    with codecs.open(filename, "r", encoding) as source:
        with codecs.open(tmpfilename, "w", "utf-8") as target:
            while True:
                contents = source.read(blocksize)
                if not contents:
                    break
                target.write(contents)

    os.rename(tmpfilename, filename)

file_name = os.path.join("/content/drive/My Drive/CICIDS2017/TrafficLabelling", "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv")

_to_utf8(file_name)

df = pd.read_csv(file_name, skipinitialspace=True, on_bad_lines='skip')

print("Removing {} rows that contains only NaN values...".format(df[df.isna().all(axis=1)].shape[0]))

df = df[~ df.isna().all(axis=1)]

def _renaming_class_label(df: pd.DataFrame):
    labels = {"Web Attack \x96 Brute Force": "Web Attack-Brute Force",
              "Web Attack \x96 XSS": "Web Attack-XSS",
              "Web Attack \x96 Sql Injection": "Web Attack-Sql Injection"}

    for old_label, new_label in labels.items():
        df.Label.replace(old_label, new_label, inplace=True)

_renaming_class_label(df)

df.to_csv(file_name, index=False)

DIR_PATH = "/content/drive/My Drive/CICIDS2017/TrafficLabelling"

FILE_NAMES = ["Monday-WorkingHours.pcap_ISCX.csv",
              "Tuesday-WorkingHours.pcap_ISCX.csv",
              "Wednesday-workingHours.pcap_ISCX.csv",
              "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv",
              "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv",
              "Friday-WorkingHours-Morning.pcap_ISCX.csv",
              "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv",
              "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv"]
df = [pd.read_csv(os.path.join(DIR_PATH, f), skipinitialspace=True) for f in FILE_NAMES]
df = pd.concat(df, ignore_index=True)
df.Label.value_counts()

df.to_csv(os.path.join(DIR_PATH, "TrafficLabelling.csv"), index=False)

import os
import pandas as pd

file_name = os.path.join("/content/drive/My Drive/CICIDS2017/MachineLearningCSV/MachineLearningCVE", "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv")

df = pd.read_csv(file_name, skipinitialspace=True, on_bad_lines='skip')

def _renaming_class_label(df: pd.DataFrame):
    labels = {"Web Attack � Brute Force": "Web Attack-Brute Force",
              "Web Attack � XSS": "Web Attack-XSS",
              "Web Attack � Sql Injection": "Web Attack-Sql Injection"}

    for old_label, new_label in labels.items():
        df.Label.replace(old_label, new_label, inplace=True)

_renaming_class_label(df)

df.to_csv(file_name, index=False)

DIR_PATH = "/content/drive/My Drive/CICIDS2017/MachineLearningCSV/MachineLearningCVE"

FILE_NAMES = ["Monday-WorkingHours.pcap_ISCX.csv",
              "Tuesday-WorkingHours.pcap_ISCX.csv",
              "Wednesday-workingHours.pcap_ISCX.csv",
              "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv",
              "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv",
              "Friday-WorkingHours-Morning.pcap_ISCX.csv",
              "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv",
              "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv"]

df = [pd.read_csv(os.path.join(DIR_PATH, f), skipinitialspace=True) for f in FILE_NAMES]
df = pd.concat(df, ignore_index=True)
df.Label.value_counts()

df.to_csv(os.path.join(DIR_PATH, "MachineLearningCVE.csv"), index=False)

import logging
import numpy as np
import pandas as pd

logging.basicConfig(format="%(asctime)s %(levelname)s %(message)s", datefmt="%H:%M:%S", level=logging.INFO)

pd.set_option("display.max_rows", 85)

TRAFFICLABELLING_PATH = "/content/drive/My Drive/CICIDS2017/TrafficLabelling/TrafficLabelling.csv"
MACHINELEARNINGCVE_PATH = "/content/drive/My Drive/CICIDS2017/MachineLearningCSV/MachineLearningCVE/MachineLearningCVE.csv"

TrafficLabelling = pd.read_csv(TRAFFICLABELLING_PATH, skipinitialspace=True)
MachineLearningCVE = pd.read_csv(MACHINELEARNINGCVE_PATH, skipinitialspace=True)

TrafficLabelling.Label.value_counts()

MachineLearningCVE.Label.value_counts()

TrafficLabelling.shape

MachineLearningCVE.shape

np.setdiff1d(TrafficLabelling.columns, MachineLearningCVE.columns) #np is not defined

TrafficLabelling.Protocol.value_counts()

import os
import logging

import pandas as pd
import numpy as np

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

logging.basicConfig(format="%(asctime)s %(levelname)s %(message)s", datefmt="%H:%M:%S", level=logging.INFO)

pd.set_option("display.max_rows", 85)

DIR_PATH = "/content/drive/My Drive/CICIDS2017/MachineLearningCSV/MachineLearningCVE"
PROCESSED_DIR_PATH = "/content/drive/My Drive/CICIDS2017/ProcessedDataset"
FILE_PATH = "/content/drive/My Drive/CICIDS2017/MachineLearningCSV/MachineLearningCVE/MachineLearningCVE.csv"

if os.path.isfile(FILE_PATH):
    print(f"File exists: {FILE_PATH}")
else:
    print(f"File not found: {FILE_PATH}")
import pandas as pd

try:
    labels = pd.read_csv(FILE_PATH, usecols=['Label'], skipinitialspace=True)
    print(labels.head())
    print(f"Shape: {labels.shape}")
except Exception as e:
    print(f"Error: {e}")

if not os.path.exists(PROCESSED_DIR_PATH):
    os.makedirs(PROCESSED_DIR_PATH)
    print(f"Created directory: {PROCESSED_DIR_PATH}")
else:
    print(f"Directory already exists: {PROCESSED_DIR_PATH}")


import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s %(message)s",
    datefmt="%H:%M:%S",
    level=logging.INFO,
)

logging.info("Reading file...")

def _label_encoding() -> LabelEncoder:
    # Create Label Encoder
    le = LabelEncoder()

    try:

        print("Reading file...")
        labels = pd.read_csv(FILE_PATH, usecols=['Label'], skipinitialspace=True)
        print(f"Total rows read: {labels.shape}")

        le.fit(labels.Label)
        print(f"Labels fitted: {len(le.classes_)} classes found.")

        np.save(os.path.join(PROCESSED_DIR_PATH, 'label_encoder.npy'), le.classes_)
        print("Label encoder saved.")

        print(f"Class distribution:\n{labels.Label.value_counts()}\n")
    except Exception as e:
        print(f"Error in _label_encoding(): {e}")
        raise

    return le



def _process(df: pd.DataFrame, le: LabelEncoder) -> pd.DataFrame:
    # Label encoding
    df.Label = le.transform(df.Label)

    # Fill NaN values
    nan_rows = df[df.isna().any(axis=1)].shape[0]
    logging.info(f"Fill NaN in {nan_rows} rows with average value of each class.")
    df.iloc[:, df.columns != "Label"] = df.groupby("Label").transform(lambda x: x.fillna(x.mean()))

    # Replace Inf values
    inf_rows = df[df.isin([np.inf]).any(axis=1)].shape[0]
    logging.info(f"Replace Inf in {inf_rows} rows with maximum value of each class.")
    df.replace([np.inf], np.nan, inplace=True)
    df.iloc[:, df.columns != "Label"] = df.groupby("Label").transform(lambda x: x.fillna(x.max()))

    # Replace negative values
    logging.info("Replace negative values with minimum value of each class.")
    df[df < 0] = np.nan
    df.iloc[:, df.columns != "Label"] = df.groupby("Label").transform(lambda x: x.fillna(x.min()))

    return df


def _split_train_test(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):
    # Sampling the dataset
    x = df.iloc[:, df.columns != 'Label']
    y = df[['Label']]

    x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.20,   #Created training and test sets
                                                        random_state=np.random.randint(10))

    del x, y

    train = pd.concat([x_train, y_train], axis=1, sort=False)
    test = pd.concat([x_test, y_test], axis=1, sort=False)

    return train, test


def _to_csv(df: pd.DataFrame, saving_path: str):
    # if file does not exist write header
    if not os.path.isfile(saving_path):
        df.to_csv(saving_path, index=False)
    # else it exists so append without writing the header
    else:
        df.to_csv(saving_path, index=False, mode='a', header=False)


def _preprocessing_all(le: LabelEncoder, chunksize=2500000):  # chunk size does not need to be 2.5million, the bigger the chunk size, the more memory is uses at a time.
    for chunk in pd.read_csv(FILE_PATH, skipinitialspace=True, chunksize=chunksize):
        print(f"Processing chunk with shape: {chunk.shape}")
        train, test = _split_train_test(_process(chunk, le))
        _to_csv(train, os.path.join(PROCESSED_DIR_PATH, "train_MachineLearningCVE.csv"))
        print("Saved train chunk.")
        _to_csv(test, os.path.join(PROCESSED_DIR_PATH, "test_MachineLearningCVE.csv"))
        print("Saved test chunk.")


label_encoder = _label_encoding()

_preprocessing_all(label_encoder, 2500000)

logging.info("*** END ***")

!pip install pytorch-lightning torchviz seaborn

# Mount your Google Drive
from google.colab import drive
drive.mount("/content/drive")

import os
import logging
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from typing import Tuple
import pytorch_lightning as pl
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint
from torchviz import make_dot
from IPython.display import Image, display
import seaborn as sns

###############################################################################
# 1. Set seeds for reproducibility
###############################################################################
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)  # if using GPUs

###############################################################################
# Logging and Pandas display configuration
###############################################################################
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(message)s",
    datefmt="%H:%M:%S",
    level=logging.INFO
)
pd.set_option("display.max_rows", 85)

###############################################################################
# Data Preprocessing
###############################################################################
def preprocessing(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)      # Shuffle and reset index
    x = df.loc[:, df.columns != 'Label']                                  # Separate features
    y = df[['Label']].values.flatten()                                    # Separate labels
    scaler = MinMaxScaler()                                               # Scale features to [0, 1]
    x = scaler.fit_transform(x)                                           # Fit scaler on features and transform
    return x, y                                                           # return scaled features and labels

###############################################################################
# Callback to log and store training metrics
###############################################################################
class MetricsLogger(pl.Callback):
    def __init__(self):
        super().__init__()                                                                      # Initialize parent Callback class
        self.history = {"train_loss": [], "train_acc": [], "val_loss": [], "val_acc": []}       # Store history for training and validation metrics

    def on_train_epoch_end(self, trainer, pl_module):
        metrics = trainer.callback_metrics                                                         # Get metrics from the current epoch

        self.history["train_loss"].append(                                                         # Add training loss for the epoch to history
            metrics.get("train_loss").item() if metrics.get("train_loss") is not None else None
        )
        self.history["train_acc"].append(                                                          # Add training accuracy for the epoch to history
            metrics.get("train_acc").item() if metrics.get("train_acc") is not None else None
        )

    def on_validation_epoch_end(self, trainer, pl_module):
        metrics = trainer.callback_metrics                                                         # Get metrics from validation epoch
        self.history["val_loss"].append(
            metrics.get("val_loss").item() if metrics.get("val_loss") is not None else None        # Add validation loss for epoch to history.
        )
        self.history["val_acc"].append(                                                            # Add validation accuracy for epoch to history
            metrics.get("val_acc").item() if metrics.get("val_acc") is not None else None
        )

###############################################################################
# Plot training history
###############################################################################
def plot_history(history: dict):
    # Plot training and validation accuracy over epochs
    plt.figure(figsize=(8, 6))
    plt.plot(history["train_acc"], label="Train Accuracy", marker="o")
    plt.plot(history["val_acc"], label="Validation Accuracy", marker="o")
    plt.title("Model Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend(loc="upper left")
    plt.grid(True)
    plt.show()

    # Plot training and validation loss over epochs
    plt.figure(figsize=(8, 6))
    plt.plot(history["train_loss"], label="Train Loss", marker="o")
    plt.plot(history["val_loss"], label="Validation Loss", marker="o")
    plt.title("Model Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend(loc="upper left")
    plt.grid(True)
    plt.show()

###############################################################################
# Evaluation with confusion matrix
###############################################################################
def evaluation(model: pl.LightningModule, dataloader: DataLoader):
    model.eval()
    device = next(model.parameters()).device  # Model's current device

    all_preds = []
    all_targets = []
    with torch.no_grad():
        for x, y in dataloader:
            # Move data to the same device as the model to avoid CPU/GPU mismatch
            x = x.to(device)
            y = y.to(device)
            logits = model(x)
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(y.cpu().numpy())

    print("Evaluation:\n{}".format(classification_report(all_targets, all_preds)))

    # Confusion matrix
    cm = confusion_matrix(all_targets, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

###############################################################################
# Define Focal Loss
###############################################################################
class FocalLoss(nn.Module):
    """
    Focal Loss as described in https://arxiv.org/abs/1708.02002
    Default: alpha=1.0, gamma=2.0
    """
    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha                                            # Weighting factor for balancing
        self.gamma = gamma                                            # Focusing parameter for hard examples
        self.reduction = reduction                                    # Reduction method: 'mean', 'sum', or 'none'
        self.cross_entropy = nn.CrossEntropyLoss(reduction='none')    # Compute loss per sample

    def forward(self, inputs, targets):
        ce_loss = self.cross_entropy(inputs, targets)           # Compute the per-sample cross-entropy loss
        p_t = torch.exp(-ce_loss)                               # Compute the probability of the true class for each sample
        modulating_factor = (1 - p_t) ** self.gamma             # Compute the modulating factor for each sample
        focal_loss = self.alpha * modulating_factor * ce_loss   # Compute the focal loss by scaling the cross-entropy loss

        if self.reduction == 'sum':
            return focal_loss.sum()
        elif self.reduction == 'none':
            return focal_loss
        else:
            return focal_loss.mean()      # Default to mean reduction


###############################################################################
# PyTorch Lightning Model
###############################################################################
class DenseClassifier(pl.LightningModule):
    def __init__(self, input_dim=78, num_classes=15, lr=1e-3):
        super(DenseClassifier, self).__init__()
        self.save_hyperparameters()           # Save hyperparameters (input_dim, num_classes, lr) for reproducibility

        # Define individual layers of the network:
        self.fc1 = nn.Linear(input_dim, 128)        # Fully connected layer: input -> 128 neurons
        self.relu1 = nn.ReLU()                      # ReLU activation, introduces non-linearity while maintaining computational efficiency

        self.fc2 = nn.Linear(128, 64)
        self.relu2 = nn.ReLU()

        self.fc3 = nn.Linear(64, 32)
        self.relu3 = nn.ReLU()

        self.fc4 = nn.Linear(32, num_classes)       # Output layer: 32 -> num_classes

        # Combine all layers to sequential model for streamlined forward pass
        self.net = nn.Sequential(
            self.fc1, self.relu1,
            self.fc2, self.relu2,
            self.fc3, self.relu3,
            self.fc4
        )

        self.loss_fn = FocalLoss(alpha=1.0, gamma=2.0)     # Set loss function to FocalLoss for class imbalance

    def forward(self, x):
        return self.net(x)      # Pass input through sequential layers

    def step(self, batch, stage="train"):
        inputs, labels = batch                                # Unpack batch into inputs and labels

        logits = self(inputs)                                 # Forward pass: compute raw output (logits)
        loss = self.loss_fn(logits, labels)                   # Compute loss using FocalLoss

        predictions = torch.argmax(logits, dim=1)             # Determine predicted class for each sample
        accuracy = (predictions == labels).float().mean()     # Compute accuracy

        self.log(f"{stage}_loss", loss, on_epoch=True, prog_bar=True)      # Log loss with prefix based stage train
        self.log(f"{stage}_acc", accuracy, on_epoch=True, prog_bar=True)   # Log accuracy with prefix based stage val

        return loss          # Return loss for backpropagation in training

    def training_step(self, batch, batch_idx):
        return self.step(batch, stage="train")      # Processes single training batch

    def validation_step(self, batch, batch_idx):
        self.step(batch, stage="val")               # Processes single validation batch

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)     # Initialize Adam optimizer with learning rate from hyperparameters


        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(                 # Define scheduler to reduce learning rate when validation accuracy plateaus
            optimizer, mode="max", factor=0.1, patience=5, verbose=True
        )

        return {                                       # Return optimizer and scheduler in dictionary format
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": "val_acc",                  # Scheduler monitors validation accuracy to adjust learning rate
                "interval": "epoch",
                "frequency": 1,
            }
        }

###############################################################################
# Main code
###############################################################################
PROCESSED_DIR_PATH = "/content/drive/My Drive/CICIDS2017/ProcessedDataset"
MODEL_DIR_PATH = "/content/drive/My Drive/CICIDS2017/Model"


def graph_visualize(model, input_dim=78, output_filename="model_arch"):
    dummy_input = torch.randn(1, input_dim)                           # Create a dummy input tensor
    dot = make_dot(model(dummy_input), params=dict(model.named_parameters()))
    dot.format = "png"
    dot.render(output_filename, cleanup=True)
    display(Image(f"{output_filename}.png"))                          # Display the generated image


def load_and_prepare_data():
    csv_file = os.path.join(PROCESSED_DIR_PATH, 'train_MachineLearningCVE.csv')     # Load the training CSV file
    df = pd.read_csv(csv_file, skipinitialspace=True)                               # Read csv file
    logging.info("Class distribution:\n{}".format(df.Label.value_counts()))         # Log class distribution

    X, y = preprocessing(df)
    del df                      # Free up memory, delete DataFrame

    # Convert processed data into PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)
    return X_tensor, y_tensor


def data_loaders(X_tensor, y_tensor, batch_size=1024):
    train_X, val_X, train_y, val_y = train_test_split(                          # Split data: 90% for training, 10% for validation
        X_tensor, y_tensor, test_size=0.1, random_state=SEED, stratify=y_tensor
    )

    train_dataset = TensorDataset(train_X, train_y)                   # Create TensorDataset objects for train sets
    val_dataset = TensorDataset(val_X, val_y)                         # Create TensorDataset objects for validation sets


    train_labels = train_y.numpy()
    class_counts = np.bincount(train_labels)                          # Calculate class counts for training labels
    logging.info("Training class counts: {}".format(class_counts))

    class_weights = 1.0 / (class_counts ** 0.4)                       # Calculate class_weight
    sample_weights = class_weights[train_labels]                      # Assign weigth to each sample based on class label
    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)              # Create WeightedRandomSampler for training Dataloader

    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)         # Create DataLoaders for training datasets
    val_loader = DataLoader(val_dataset, batch_size=batch_size)                              # Create DataLoaders for validation datasets
    return train_loader, val_loader                                                          # Return DataLoaders with weighted sampling


def configure_callbacks():
    metrics_logger = MetricsLogger()                          # Logs training and validation metrics.
    checkpoint_callback = pl.callbacks.ModelCheckpoint(       # Saves the best model based on validation accuracy
        dirpath = MODEL_DIR_PATH,
        filename = "best_dense_{epoch:02d}-{val_acc:.4f}",
        monitor = "val_acc",
        mode = "max",
        save_top_k = 1
    )
    return metrics_logger, checkpoint_callback


def main():

    model = DenseClassifier()                                   # Create an instance of the model
    logging.info("Model architecture:\n{}".format(model))

    graph_visualize(model, input_dim=78)           # Visualize the model architecture

    X_tensor, y_tensor = load_and_prepare_data()                # Load, preprocess, and convert training data into tensors

    train_loader, val_loader = data_loaders(X_tensor, y_tensor, batch_size=1024)    # Create DataLoaders for training and validation

    metrics_logger, checkpoint_callback = configure_callbacks()       # Set up callbacks for logging and checkpointing

    trainer = pl.Trainer(                             # Configure PyTorch Lightning Trainer
        max_epochs=125,
        callbacks=[metrics_logger, checkpoint_callback],
        log_every_n_steps=10,
        accelerator='auto'
    )

    logging.info("*** TRAINING START ***")
    trainer.fit(model, train_loader, val_loader)
    logging.info("*** TRAINING FINISH ***")

    global best_model
    best_checkpoint_path = checkpoint_callback.best_model_path                  # Retrieve and log path of best model checkpoint
    logging.info(f"Best model saved at: {best_checkpoint_path}")
    best_model = DenseClassifier.load_from_checkpoint(best_checkpoint_path)     # Load best model from checkpoint

    plot_history(metrics_logger.history)            # Plot training history (accuracy and loss over epochs)


    eval_dataset = TensorDataset(X_tensor, y_tensor)            # Evaluate dataset
    eval_loader = DataLoader(eval_dataset, batch_size=1024)     # Evaluate data loader
    evaluation(best_model, eval_loader)                         # Evaluate the model on the entire dataset


# Entry point for the script
if __name__ == "__main__":
    main()

###############################################################################
# LABEL POST EVALUATION CSV
###############################################################################
def label_csv(model: pl.LightningModule, input_csv_path: str, output_csv_path: str, class_mapping: dict) -> pd.DataFrame:
    df = pd.read_csv(input_csv_path)              # Load the CSV file

    numeric_columns = df.select_dtypes(include=['number']).columns      # Select only numeric columns (features)
    unlabeled_features = df[numeric_columns]                            # Create new DataFrame containing only numeric features.

    #unlabeled_features = unlabeled_features.replace([np.inf, -np.inf], np.nan).fillna(0)    # Replace infinite values with NaN and then fill NaNs with 0.


    if unlabeled_features.shape[1] != model.hparams.input_dim:          # Ensure number of features matches model's input dimension
        raise ValueError(
            f"Number of features in the dataset ({unlabeled_features.shape[1]}) does not match "
            f"the model's input dimension ({model.hparams.input_dim})."
        )


    scaler = MinMaxScaler()                                     # Create instance
    X_scaled = scaler.fit_transform(unlabeled_features)         # Preprocess data (scale to [0,1])

    X_tensor = torch.tensor(X_scaled, dtype=torch.float32)      # Convert to PyTorch tensor

    model.eval()                                                # Ensure model in evaluation mode
    device = next(model.parameters()).device                    # Get model device
    X_tensor = X_tensor.to(device)                              # Move tensor to same device as the model

    with torch.no_grad():                                       # Predict labels
        logits = model(X_tensor)
        predictions = torch.argmax(logits, dim=1).cpu().numpy()

    df["Predicted_Label"] = [class_mapping[pred] for pred in predictions]     # Map predicted class indices to class names

    df.to_csv(output_csv_path, index=False)               # Save labeled CSV
    print(f"Labeled dataset saved to: {output_csv_path}")
    return df

###############################################################################
# HELPER FUNCTION: keep columns found in training set
###############################################################################
def drop_unwanted_columns(df: pd.DataFrame, train_cols: list) -> pd.DataFrame:
    common_cols = [c for c in df.columns if c in train_cols]        # Intersect columns with train_cols

    df = df[common_cols]                                            # Re-order exactly as train_cols (dropping any missing)

    missing_cols = set(train_cols) - set(common_cols)               # Identify missing columns
    for col in missing_cols:                                        # Fill missing columns with 0
        df.loc[:, col] = 0
    df = df[train_cols]                                             # Re-order exactly as train_cols (dropping any missing)
    return df

###############################################################################
# MAIN LOOP
###############################################################################
unlabeled_folder = "/content/drive/My Drive/CSV_UNLABELED"
labeled_folder   = "/content/drive/My Drive/CSV_LABELED"

train_df = pd.read_csv(                                                 # Load the training dataset
    os.path.join(PROCESSED_DIR_PATH, 'train_MachineLearningCVE.csv'),
    skipinitialspace=True
)

train_columns = train_df.drop(columns=['Label']).columns.tolist()     # Grab columns used by model (exclude 'Label')

column_mapping = {                      # Column rename dictionary
    "Src Port": "Source Port",
    "Dst Port": "Destination Port",
    "Tot Fwd Pkts": "Total Fwd Packets",
    "Tot Bwd Pkts": "Total Backward Packets",
    "TotLen Fwd Pkts": "Total Length of Fwd Packets",
    "TotLen Bwd Pkts": "Total Length of Bwd Packets",
    "Fwd Pkt Len Max": "Fwd Packet Length Max",
    "Fwd Pkt Len Min": "Fwd Packet Length Min",
    "Fwd Pkt Len Mean": "Fwd Packet Length Mean",
    "Fwd Pkt Len Std": "Fwd Packet Length Std",
    "Bwd Pkt Len Max": "Bwd Packet Length Max",
    "Bwd Pkt Len Min": "Bwd Packet Length Min",
    "Bwd Pkt Len Mean": "Bwd Packet Length Mean",
    "Bwd Pkt Len Std": "Bwd Packet Length Std",
    "Flow Byts/s": "Flow Bytes/s",
    "Flow Pkts/s": "Flow Packets/s",
    "Fwd IAT Tot": "Fwd IAT Total",
    "Bwd IAT Tot": "Bwd IAT Total",
    "Fwd Header Len": "Fwd Header Length",
    "Bwd Header Len": "Bwd Header Length",
    "Fwd Pkts/s": "Fwd Packets/s",
    "Bwd Pkts/s": "Bwd Packets/s",
    "Pkt Len Min": "Min Packet Length",
    "Pkt Len Max": "Max Packet Length",
    "Pkt Len Mean": "Packet Length Mean",
    "Pkt Len Std": "Packet Length Std",
    "Pkt Len Var": "Packet Length Variance",
    "FIN Flag Cnt": "FIN Flag Count",
    "SYN Flag Cnt": "SYN Flag Count",
    "RST Flag Cnt": "RST Flag Count",
    "PSH Flag Cnt": "PSH Flag Count",
    "ACK Flag Cnt": "ACK Flag Count",
    "URG Flag Cnt": "URG Flag Count",
    "CWE Flag Count": "CWE Flag Count",
    "ECE Flag Cnt": "ECE Flag Count",
    "Pkt Size Avg": "Average Packet Size",
    "Fwd Seg Size Avg": "Avg Fwd Segment Size",
    "Bwd Seg Size Avg": "Avg Bwd Segment Size",
    "Fwd Byts/b Avg": "Fwd Avg Bytes/Bulk",
    "Fwd Pkts/b Avg": "Fwd Avg Packets/Bulk",
    "Fwd Blk Rate Avg": "Fwd Avg Bulk Rate",
    "Bwd Byts/b Avg": "Bwd Avg Bytes/Bulk",
    "Bwd Pkts/b Avg": "Bwd Avg Packets/Bulk",
    "Bwd Blk Rate Avg": "Bwd Avg Bulk Rate",
    "Subflow Fwd Pkts": "Subflow Fwd Packets",
    "Subflow Fwd Byts": "Subflow Fwd Bytes",
    "Subflow Bwd Pkts": "Subflow Bwd Packets",
    "Subflow Bwd Byts": "Subflow Bwd Bytes",
    "Init Fwd Win Byts": "Init_Win_bytes_forward",
    "Init Bwd Win Byts": "Init_Win_bytes_backward",
    "Fwd Act Data Pkts": "act_data_pkt_fwd",
    "Fwd Seg Size Min": "min_seg_size_forward"
}

class_mapping = {             # Class mapping for final predictions
    0:  "Benign",
    1:  "Bot",
    2:  "DDoS",
    3:  "DoS GoldenEye",
    4:  "DoS Hulk",
    5:  "DoS Slowhttptest",
    6:  "DoS slowloris",
    7:  "FTP-Patator",
    8:  "Heartbleed",
    9:  "Infiltration",
    10: "PortScan",
    11: "SSH-Patator",
    12: "Web Attack-Brute Force",
    13: "Web Attack-Sql Injection",
    14: "Web Attack-XSS"
}

for filename in os.listdir(unlabeled_folder):
    if filename.lower().endswith(".csv"):
        input_csv_path = os.path.join(unlabeled_folder, filename)
        base_name = os.path.splitext(filename)[0]
        output_csv_path = os.path.join(labeled_folder, base_name + "_labeled.csv")

        df = pd.read_csv(input_csv_path)                    # Load the unlabeled dataset
        df = df.rename(columns=column_mapping)              # Rename columns if they appear in your dictionary
        df = drop_unwanted_columns(df, train_columns)       # Drop colums not in train_columns and ensure they are in correct order.
        df.to_csv(input_csv_path, index=False)              # Overwrite CSV with the corrected columns
        labeled_df = label_csv(best_model, input_csv_path, output_csv_path, class_mapping)     # Label dataset
